{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path: /Users/tx44ru/Projects/Kaggle/DisasterTweets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Repo path:\", repo_path)\n",
    "except:\n",
    "    os.chdir(\"../\")\n",
    "    repo_path = os.getcwd()\n",
    "    print(\"Repo path:\", repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "use_dir = \"use\"\n",
    "glove_dir = \"glove\"\n",
    "\n",
    "os.chdir(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv', 'train.csv', 'sample_submission.csv']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "# df[\"text_token\"] = df[\"text\"].apply(lambda x: [token.lower() for token in x.split()])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'shapent\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"shapent\"\n",
    "# word  = \"\".join([char.lower() if char not in string.punctuation else \" \" for char in word ]).split()\n",
    "# word\n",
    "\"'\"+word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rockyfire, update, california, hwy, closed, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>[flood, disaster, heavy, rain, causes, flash, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, 'm, on, top, of, the, hill, and, i, can, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, 's, an, emergency, evacuation, happeni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, 'm, afraid, that, the, tornado, is, coming...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target                                         text_token  \n",
       "0       1  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1       1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2       1  [all, residents, asked, to, shelter, in, place...  \n",
       "3       1  [people, receive, wildfires, evacuation, order...  \n",
       "4       1  [just, got, sent, this, photo, from, ruby, ala...  \n",
       "5       1  [rockyfire, update, california, hwy, closed, i...  \n",
       "6       1  [flood, disaster, heavy, rain, causes, flash, ...  \n",
       "7       1  [i, 'm, on, top, of, the, hill, and, i, can, s...  \n",
       "8       1  [there, 's, an, emergency, evacuation, happeni...  \n",
       "9       1  [i, 'm, afraid, that, the, tornado, is, coming...  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def txt_cleanning(text, tokenize=True, stem=False):\n",
    "    \"\"\" cleanning:\n",
    "            lowercase\n",
    "            remove numbers\n",
    "            remove punctuation\n",
    "            stem words\n",
    "    \"\"\"\n",
    "    if not isinstance(text, list):\n",
    "        # workaround in order to be able to re-run it\n",
    "        text  = \"\".join([char.lower() for char in text ])\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        text = text.split()\n",
    "        out_text = []\n",
    "        for word in text:\n",
    "            if \"\\\\x\" in word:\n",
    "                continue\n",
    "            if \"http\" in word:\n",
    "                out_text.append(\"http\")\n",
    "                continue\n",
    "            if word == \"ain't\":\n",
    "                out_text += [\"is\", \"not\"]\n",
    "                continue\n",
    "            if word == \"won't\":\n",
    "                out_text += [\"will\", \"not\"]\n",
    "                continue\n",
    "            if word and word[0] in string.punctuation:\n",
    "                word = word[1:]\n",
    "            if word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "            if len(word) > 4 and \"n't\" in word[-3:]:\n",
    "                out_text.append(word[:-3])\n",
    "                out_text.append(\"not\")\n",
    "            elif \"'\" in word:\n",
    "                new_words = word.split(\"'\")\n",
    "                out_text += [new_words[0], \"'\" + new_words[1]]\n",
    "            elif \"-\" in word:\n",
    "                out_text += word.split(\"-\")\n",
    "            elif word:\n",
    "                word = re.sub(\"[^a-z]+\", \"\", word)\n",
    "                out_text  += \"\".join([char if char.isalpha()\n",
    "                                              else \" \" for char in word ]).split()\n",
    "    out_text = [word.strip() for word in out_text if word.strip()]\n",
    "    if stem:\n",
    "        text = [ps.stem(word) for word in out_text]  # remove stopwords and stemming\n",
    "    if not tokenize:\n",
    "        out_text = \" \".join([str(elem) for elem in out_text])\n",
    "    return out_text\n",
    "\n",
    "df['text_token'] = df['text'].apply(lambda x: txt_cleanning(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(inpt, returnString=False):\n",
    "    if not isinstance(inpt, list):\n",
    "        inpt = inpt.split()\n",
    "    inpt = [token.strip() for token in inpt]\n",
    "    ret = [token[1:] if len(token) > 1 and token[0] == \"#\" else token for token in inpt]\n",
    "    if returnString:\n",
    "        return \" \".join([token for token in ret])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"text_token\"] = df[\"text_token\"].apply(clean_hashtag)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_df, y_df = df[\"text\"], df[\"target\"]\n",
    "X_df, y_df = df[\"text_token\"], df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (5709,)\n",
      "test: (1904,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", X_train.shape)\n",
    "print(\"test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5244    [refugio, oil, spill, may, have, been, costlie...\n",
       "4860    [julian, knight, scvsupremecourt, dismisses, m...\n",
       "6538    [electricity, cant, stop, scofield, nigga, sur...\n",
       "5175    [meek, mill, begging, nicki, minaj, to, let, h...\n",
       "5820    [chinas, stock, market, crash, this, summer, h...\n",
       "Name: text_token, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE module:\n",
      " <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7fd439ec6550>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    print(\"USE module:\\n\", use_module)\n",
    "except:\n",
    "    use_module = hub.load(use_dir)\n",
    "    print(\"USE module:\\n\", use_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_use(text, use_module):\n",
    "    text = [text]\n",
    "    emb = use_module.signatures[\"response_encoder\"](\n",
    "        input=tf.constant(text),\n",
    "        context=tf.constant(text))[\"outputs\"].numpy()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"This is a test\"\n",
    "get_use(test, use_module).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5709/5709 [03:14<00:00, 29.40it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_use = []\n",
    "\n",
    "for text in tqdm(X_train):\n",
    "    X_train_use.append(get_use(text, use_module).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1904/1904 [01:04<00:00, 29.55it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_use = []\n",
    "\n",
    "for text in tqdm(X_test):\n",
    "    X_test_use.append(get_use(text, use_module).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_use[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_use = []\n",
    "\n",
    "for text in tqdm(X_train):\n",
    "    X_train_use.append(get_use(text, use_module).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the vectorizer method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(min_df=3, max_df=0.8, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7b74298b67d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m   1630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf_vec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vec.transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vec.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 3372)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings imported.\n",
      "Vector size: \n",
      "200\n",
      "\n",
      "CPU times: user 31.8 s, sys: 8.74 s, total: 40.5 s\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_voc = []\n",
    "glove_weights = []\n",
    "glove_dimensions = 200\n",
    "glove_file = os.path.join(glove_dir, \"glove.twitter.27B.{}d.txt\".format(glove_dimensions))\n",
    "with open(glove_file, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        if len(values[1:]) == glove_dimensions:\n",
    "            glove_voc.append(values[0])\n",
    "            glove_weights.append(values[1:])\n",
    "print(\"GloVe embeddings imported.\\nVector size: \")\n",
    "print(len(glove_weights[-1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights = np.asarray(glove_weights, \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "(glove_voc_length, glove_weights_length) = glove_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input = X_train_use\n",
    "X_test_input = X_test_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_input, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7478991596638656"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the model's accuracy\n",
    "nb.score(X_test_input, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Net classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe8cc7060d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, round(input_size / 2))\n",
    "        self.fc2 = nn.Linear(round(input_size / 2), 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        #Apply sigmoid to output.\n",
    "        pred = self.forward(x)\n",
    "        return pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deeper NN\n",
    "class SimpleNet_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNet_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.dropout2 = nn.Dropout(p = 0.3)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = torch.sigmoid(self.fc4(x))\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        #Apply sigmoid to output.\n",
    "        pred = self.forward(x)\n",
    "        return pred.round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input = X_train_use\n",
    "X_test_input = X_test_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimension(inp):\n",
    "    if isinstance(inp, list):\n",
    "        return len(inp[0])\n",
    "    if isinstance(inp, np.ndarray):\n",
    "        return inp.shape[1]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_classifier = SimpleNet_v2(get_dimension(X_train_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, scheduler, name):\n",
    "    train_state = {    \n",
    "    'model' : model,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(train_state, name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(simple_classifier.parameters(), lr=0.001, weight_decay = 0.007);\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'max', factor = 0.1, patience = 5, verbose=True)\n",
    "\n",
    "# train_loader, test_loader = create_dataloaders(balanced_train_images, balanced_train_labels, pr_test_img, pr_test_labels)\n",
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 0.021\n",
      "Accuracy on EPOCH 1 test images: 58 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.581408\n",
      "[2,    30] loss: 0.019\n",
      "Accuracy on EPOCH 2 test images: 81 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.815126\n",
      "[3,    30] loss: 0.015\n",
      "Accuracy on EPOCH 3 test images: 80 %\n",
      "[4,    30] loss: 0.013\n",
      "Accuracy on EPOCH 4 test images: 81 %\n",
      "[5,    30] loss: 0.013\n",
      "Accuracy on EPOCH 5 test images: 81 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.819328\n",
      "[6,    30] loss: 0.013\n",
      "Accuracy on EPOCH 6 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.820903\n",
      "[7,    30] loss: 0.013\n",
      "Accuracy on EPOCH 7 test images: 81 %\n",
      "[8,    30] loss: 0.013\n",
      "Accuracy on EPOCH 8 test images: 81 %\n",
      "[9,    30] loss: 0.012\n",
      "Accuracy on EPOCH 9 test images: 81 %\n",
      "[10,    30] loss: 0.013\n",
      "Accuracy on EPOCH 10 test images: 82 %\n",
      "[11,    30] loss: 0.013\n",
      "Accuracy on EPOCH 11 test images: 81 %\n",
      "[12,    30] loss: 0.013\n",
      "Accuracy on EPOCH 12 test images: 82 %\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[13,    30] loss: 0.012\n",
      "Accuracy on EPOCH 13 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.824055\n",
      "[14,    30] loss: 0.012\n",
      "Accuracy on EPOCH 14 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.824580\n",
      "[15,    30] loss: 0.012\n",
      "Accuracy on EPOCH 15 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.825105\n",
      "[16,    30] loss: 0.012\n",
      "Accuracy on EPOCH 16 test images: 82 %\n",
      "[17,    30] loss: 0.012\n",
      "Accuracy on EPOCH 17 test images: 82 %\n",
      "[18,    30] loss: 0.012\n",
      "Accuracy on EPOCH 18 test images: 82 %\n",
      "[19,    30] loss: 0.012\n",
      "Accuracy on EPOCH 19 test images: 82 %\n",
      "[20,    30] loss: 0.012\n",
      "Accuracy on EPOCH 20 test images: 82 %\n",
      "[21,    30] loss: 0.012\n",
      "Accuracy on EPOCH 21 test images: 82 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[22,    30] loss: 0.011\n",
      "Accuracy on EPOCH 22 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.825630\n",
      "[23,    30] loss: 0.012\n",
      "Accuracy on EPOCH 23 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.826155\n",
      "[24,    30] loss: 0.012\n",
      "Accuracy on EPOCH 24 test images: 82 %\n",
      "[25,    30] loss: 0.012\n",
      "Accuracy on EPOCH 25 test images: 82 %\n",
      "[26,    30] loss: 0.012\n",
      "Accuracy on EPOCH 26 test images: 82 %\n",
      "[27,    30] loss: 0.011\n",
      "Accuracy on EPOCH 27 test images: 82 %\n",
      "[28,    30] loss: 0.011\n",
      "Accuracy on EPOCH 28 test images: 82 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.826681\n",
      "[29,    30] loss: 0.011\n",
      "Accuracy on EPOCH 29 test images: 82 %\n",
      "[30,    30] loss: 0.012\n",
      "Accuracy on EPOCH 30 test images: 82 %\n",
      "[31,    30] loss: 0.012\n",
      "Accuracy on EPOCH 31 test images: 82 %\n",
      "[32,    30] loss: 0.012\n",
      "Accuracy on EPOCH 32 test images: 82 %\n",
      "[33,    30] loss: 0.012\n",
      "Accuracy on EPOCH 33 test images: 82 %\n",
      "[34,    30] loss: 0.011\n",
      "Accuracy on EPOCH 34 test images: 82 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[35,    30] loss: 0.012\n",
      "Accuracy on EPOCH 35 test images: 82 %\n",
      "[36,    30] loss: 0.011\n",
      "Accuracy on EPOCH 36 test images: 82 %\n",
      "[37,    30] loss: 0.011\n",
      "Accuracy on EPOCH 37 test images: 82 %\n",
      "[38,    30] loss: 0.012\n",
      "Accuracy on EPOCH 38 test images: 82 %\n",
      "[39,    30] loss: 0.011\n",
      "Accuracy on EPOCH 39 test images: 82 %\n",
      "[40,    30] loss: 0.012\n",
      "Accuracy on EPOCH 40 test images: 82 %\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "[41,    30] loss: 0.012\n",
      "Accuracy on EPOCH 41 test images: 82 %\n",
      "[42,    30] loss: 0.011\n",
      "Accuracy on EPOCH 42 test images: 82 %\n",
      "[43,    30] loss: 0.012\n",
      "Accuracy on EPOCH 43 test images: 82 %\n",
      "[44,    30] loss: 0.012\n",
      "Accuracy on EPOCH 44 test images: 82 %\n",
      "[45,    30] loss: 0.012\n",
      "Accuracy on EPOCH 45 test images: 82 %\n",
      "[46,    30] loss: 0.011\n",
      "Accuracy on EPOCH 46 test images: 82 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "[47,    30] loss: 0.012\n",
      "Accuracy on EPOCH 47 test images: 82 %\n",
      "[48,    30] loss: 0.012\n",
      "Accuracy on EPOCH 48 test images: 82 %\n",
      "STOPPED EARLY!!\n",
      "Finished Training\n",
      "CPU times: user 9min 31s, sys: 5.45 s, total: 9min 37s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EARLY_STOP = 0\n",
    "EPOCH = 50\n",
    "batchsize = 128\n",
    "model_name = 'SimpleNet_use_3layers.pt'\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "    if EARLY_STOP >= 21:\n",
    "        print(\"STOPPED EARLY!!\")\n",
    "        break\n",
    "    simple_classifier.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(y_train), batchsize):\n",
    "        \n",
    "        inputs = torch.tensor(X_train_input[i:i+batchsize]).float()\n",
    "        lbs = torch.tensor(y_train[i:i+batchsize].values).float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = simple_classifier(inputs)\n",
    "        loss = criterion(outputs, lbs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i/batchsize) % 30 == 29:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, (i/batchsize) + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    simple_classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(0, len(y_test), batchsize):\n",
    "        labels = torch.tensor(y_test[j:j+batchsize].values)\n",
    "        outputs = simple_classifier.predict(torch.tensor(X_test_input[j:j+batchsize]).float())\n",
    "        total += labels.shape[0]\n",
    "        correct += outputs.squeeze().eq(labels).sum().item()\n",
    "    cur_accuracy = correct / total\n",
    "    print('Accuracy on EPOCH %d test images: %d %%' % (epoch+1, 100 * cur_accuracy))   \n",
    "    lr_scheduler.step(cur_accuracy)\n",
    "    if cur_accuracy > best_accuracy:\n",
    "            best_accuracy = cur_accuracy\n",
    "            print(\"Saving current model!!!\")\n",
    "            print(\"Detailed Accuracy: %f\" %(best_accuracy))\n",
    "            save_model(epoch, simple_classifier, optimizer, lr_scheduler, model_name)\n",
    "            EARLY_STOP = 0\n",
    "    EARLY_STOP += 1\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.PAD = 0\n",
    "        self.index2word = {self.PAD: \"PAD\"}\n",
    "        self.word2index = {\"PAD\": self.PAD}\n",
    "        self.size = 1\n",
    "        self.oov = {}\n",
    "        \n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.size\n",
    "            self.index2word[self.size] = word \n",
    "            self.size += 1\n",
    "        return self.size-1\n",
    "    \n",
    "    def add_list(self, voc_list):\n",
    "        if not isinstance(voc_list, list):\n",
    "            voc_list = voc_list.split()\n",
    "        for word in voc_list:\n",
    "            self.add_word(word)\n",
    "        return \"1\"\n",
    "\n",
    "    def get_word(self, index):\n",
    "        try:\n",
    "            return self.index2word[index]\n",
    "        except:\n",
    "            return \"Error: INDEX TOO BIG\"\n",
    "\n",
    "    def get_index(self, word):\n",
    "        try:\n",
    "            return self.word2index[word]\n",
    "        except:\n",
    "            if word not in self.oov:\n",
    "                self.oov[word] = 1\n",
    "            else:\n",
    "                self.oov[word] += 1\n",
    "            return -1\n",
    "    def res_oov(self):\n",
    "        self.oov = {}\n",
    "        return \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocabulary = Vocabulary()\n",
    "my_vocabulary.add_list(glove_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocabulary.get_index(\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'try'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_voc[553]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fccfdfa7ad0>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(glove_voc_length+1, glove_weights_length, padding_idx=0)\n",
    "embeddings.weight = nn.Parameter(torch.cat([torch.zeros(1, glove_dimensions), torch.tensor(glove_weights)], dim=0))\n",
    "embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True]]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt = torch.LongTensor([[glove_voc_length]])\n",
    "embeddings(inpt) == torch.tensor(glove_weights[glove_voc_length-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, vocabulary_length, emb_dim, emb_weights, \n",
    "                 lstm_layers=1, output_size=1, oov_id=-1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.oov_id = oov_id\n",
    "        self.embedding_dim = emb_dim\n",
    "        self.embeddings = nn.Embedding(vocabulary_length, emb_dim, padding_idx=0)\n",
    "        self.embeddings.weight = emb_weights\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = lstm_layers\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, hidden_size, lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p = 0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, sentence_arr):\n",
    "        batch_size = sentence_arr.shape[0]\n",
    "        h0 = Variable(torch.zeros(self.layers, batch_size, self.hidden_size)).requires_grad_() \n",
    "        c0 = Variable(torch.zeros(self.layers, batch_size, self.hidden_size)).requires_grad_()\n",
    "        inpt = self.create_input(sentence_arr)\n",
    "        out, (out_state, final_cell_state) = self.lstm(inpt, (h0.detach(), c0.detach()))\n",
    "        x = F.relu(out[:, -1, :])\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "        return out, (out_state, final_cell_state)\n",
    "    \n",
    "    def predict(self, sentence_arr):\n",
    "        pred, _ = self.forward(sentence_arr)\n",
    "        return pred.round()\n",
    "        \n",
    "    def create_input(self, sentence_arr):\n",
    "        \"\"\"\n",
    "        code inspired from https://stackoverflow.com/questions/53316174/\n",
    "        using-pre-trained-word-embeddings-how-to-create-vector-for-unknown-oov-token\n",
    "        \"\"\"\n",
    "        sentence_arr = sentence_arr\n",
    "        mask = (sentence_arr==self.oov_id).long()\n",
    "#         print(mask.sum())\n",
    "        embed_known = self.embeddings((1-mask)*sentence_arr)\n",
    "        embed_random = mask.unsqueeze(-1)*torch.nn.Parameter(data=torch.rand(embed_known.shape))\n",
    "        emb = embed_known + embed_random\n",
    "        emb = emb.detach()\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 80\n",
    "emb_weigths = nn.Parameter(torch.cat([torch.zeros(1, glove_dimensions), torch.tensor(glove_weights)], dim=0))\n",
    "lstm_cf = LSTMClassifier(200, glove_voc_length+1, glove_weights_length,  emb_weigths, lstm_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_cf.create_input(torch.tensor(data=[-1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_index(lst, sentence_length, vocabulary):\n",
    "    ret = [vocabulary.get_index(word) for word in lst]\n",
    "    if len(ret) < sentence_length:\n",
    "        for i in range(sentence_length - len(ret)):\n",
    "            ret.append(0)\n",
    "    else:\n",
    "        ret = ret[:sentence_length]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, vocabulary, batch_size=64, sentence_length=15):\n",
    "    X = np.vstack(X.apply(lambda x: sentence_to_index(x, sentence_length, vocabulary)))\n",
    "    dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(y.values))\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocabulary.res_oov()\n",
    "def create_numpy(X, y, vocabulary, batch_size=64, sentence_length=15):\n",
    "    X = np.vstack(X.apply(lambda x: sentence_to_index(x, sentence_length, vocabulary)))\n",
    "    return X, y.values\n",
    "\n",
    "X_train_lstm, y_train_lstm = create_numpy(X_train, y_train, my_vocabulary, batch_size=batch_size)\n",
    "X_test_lstm, y_test_lstm = create_numpy(X_test, y_test, my_vocabulary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.000000\n",
       "mean       14.697360\n",
       "std         5.817742\n",
       "min         1.000000\n",
       "25%        10.000000\n",
       "50%        15.000000\n",
       "75%        19.000000\n",
       "max        32.000000\n",
       "Name: sentence_length, dtype: float64"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentence_length\"] = df[\"text_token\"].apply(lambda x: len(x))\n",
    "df[\"sentence_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(X_train, y_train, my_vocabulary, batch_size=batch_size)\n",
    "test_dataloader = create_dataloader(X_test, y_test, my_vocabulary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bioterror': 66,\n",
       " 'prebreak': 60,\n",
       " 'soudelor': 52,\n",
       " 'bioterrorism': 48,\n",
       " 'bestnaijamade': 48,\n",
       " 'disea': 38,\n",
       " 'funtenna': 34,\n",
       " 'crematoria': 30,\n",
       " 'udhampur': 26,\n",
       " 'spos': 18,\n",
       " 'inundation': 18,\n",
       " 'irandeal': 18,\n",
       " 'mediterran': 16,\n",
       " 'icemoon': 14,\n",
       " 'djicemoon': 14,\n",
       " 'microlight': 14,\n",
       " 'linkury': 14,\n",
       " 'runion': 14,\n",
       " 'mhtw': 14,\n",
       " 'fnet': 14,\n",
       " 'animalrescue': 14,\n",
       " 'canaanites': 14,\n",
       " 'dorret': 12,\n",
       " 'wheavenly': 12,\n",
       " 'prophetmuhammad': 12,\n",
       " 'sinjar': 12,\n",
       " 'mikeparractor': 12,\n",
       " \"'all\": 12,\n",
       " 'eblokeve': 12,\n",
       " '&gt': 12,\n",
       " 'youngheroesid': 10,\n",
       " 'pantherattack': 10,\n",
       " 'twia': 10,\n",
       " 'viralspell': 10,\n",
       " 'naved': 10,\n",
       " 'sittwe': 10,\n",
       " 'strategicpatience': 10,\n",
       " 'usagov': 10,\n",
       " 'summerfate': 10,\n",
       " 'injuryi': 10,\n",
       " 'gtgtgt': 10,\n",
       " 'kerricktrial': 10,\n",
       " 'socialnews': 10,\n",
       " 'nasahurricane': 10,\n",
       " 'rockyfire': 8,\n",
       " 'wisenews': 8,\n",
       " 'waimate': 8,\n",
       " 'unsuckdcmetro': 8,\n",
       " 'carryi': 8,\n",
       " 'kurtschlichter': 8,\n",
       " 'beforeitsnews': 8,\n",
       " 'votejkt': 8,\n",
       " 'raynbowaffair': 8,\n",
       " 'diamondkesawn': 8,\n",
       " 'ramag': 8,\n",
       " 'warfighting': 8,\n",
       " 'lulgzimbestpicts': 8,\n",
       " 'rexyy': 8,\n",
       " 'xdyk': 8,\n",
       " 'worstsummerjob': 8,\n",
       " 'karymsky': 8,\n",
       " 'yazidis': 8,\n",
       " 'abbswinston': 8,\n",
       " 'realdonaldtrump': 8,\n",
       " 'idfire': 8,\n",
       " \"'..\": 8,\n",
       " 'japn': 8,\n",
       " 'pbban': 8,\n",
       " \"'armageddon\": 8,\n",
       " 'localarsonist': 8,\n",
       " 'lonewolffur': 8,\n",
       " 'otrametlife': 8,\n",
       " 'fettilootch': 8,\n",
       " 'slanglucci': 8,\n",
       " 'oppressions': 8,\n",
       " 'bluedio': 6,\n",
       " 'cbcca': 6,\n",
       " 'robotrainstorm': 6,\n",
       " 'rworldnews': 6,\n",
       " 'harmkid': 6,\n",
       " 'wratka': 6,\n",
       " 'rickperry': 6,\n",
       " 'harrybecareful': 6,\n",
       " 'bigamist': 6,\n",
       " 'dannyonpc': 6,\n",
       " 'throwingknifes': 6,\n",
       " 'kvx': 6,\n",
       " 'newsintweets': 6,\n",
       " 'taxiways': 6,\n",
       " 'navbl': 6,\n",
       " 'ihhen': 6,\n",
       " 'azwoamzk': 6,\n",
       " 'onlinecommunities': 6,\n",
       " 'nickcannon': 6,\n",
       " 'listenbuy': 6,\n",
       " 'realmandyrain': 6,\n",
       " 'itunesmusic': 6,\n",
       " 'blowmandyup': 6,\n",
       " 'tilnow': 6,\n",
       " 'saddlebrooke': 6,\n",
       " 'ultimalucha': 6,\n",
       " 'lncvu': 6,\n",
       " 'davidvonderhaar': 6,\n",
       " 'aoms': 6,\n",
       " 'ibooklove': 6,\n",
       " 'bookboost': 6,\n",
       " 'gopdebate': 6,\n",
       " 'blksamp': 6,\n",
       " 'hostageamp': 6,\n",
       " 'wbioterrorismampuse': 6,\n",
       " 'lglorg': 6,\n",
       " 'clearedincident': 6,\n",
       " 'faroeislands': 6,\n",
       " 'hatcap': 6,\n",
       " 'silvergray': 6,\n",
       " 'metrofmtalk': 6,\n",
       " 'sigalert': 6,\n",
       " 'wedaug': 6,\n",
       " 'apollobrown': 6,\n",
       " 'modiministry': 6,\n",
       " 'accionempresa': 6,\n",
       " 'gerenciatodos': 6,\n",
       " 'foodscare': 6,\n",
       " 'nestleindia': 6,\n",
       " 'magginoodle': 6,\n",
       " 'humanconsumption': 6,\n",
       " 'spinningbot': 6,\n",
       " 'mnpdnashville': 6,\n",
       " 'godslove': 6,\n",
       " 'trubgme': 6,\n",
       " 'blizzarddraco': 6,\n",
       " 'gamergate': 6,\n",
       " 'tinyjecht': 6,\n",
       " 'xsx': 6,\n",
       " \"'?\": 6,\n",
       " 'savebees': 6,\n",
       " 'ianhellfire': 6,\n",
       " 'ptsdchat': 6,\n",
       " 'tflbusalerts': 6,\n",
       " 'reshapes': 6,\n",
       " 'humaza': 6,\n",
       " 'nwajli': 6,\n",
       " 'worldnetdaily': 6,\n",
       " 'fennovoima': 6,\n",
       " 'cecilthelion': 6,\n",
       " 'blacklivesmatter': 6,\n",
       " 'zakbagans': 6,\n",
       " 'usnwsgov': 6,\n",
       " 'weathernetwork': 6,\n",
       " 'yugvani': 6,\n",
       " 'demonstratio': 6,\n",
       " 'offensivecontent': 6,\n",
       " \"'the\": 6,\n",
       " 'claytonbryant': 6,\n",
       " 'artistsunited': 6,\n",
       " 'anellatulip': 4,\n",
       " 'magisters': 4,\n",
       " 'pakistannews': 4,\n",
       " 'nankana': 4,\n",
       " 'katunews': 4,\n",
       " 'abubaraa': 4,\n",
       " 'pickerel': 4,\n",
       " 'dmpl': 4,\n",
       " 'casperrmg': 4,\n",
       " 'lmfaoooo': 4,\n",
       " 'foxnew': 4,\n",
       " 'scottwalker': 4,\n",
       " 'janaq': 4,\n",
       " 'pakpattan': 4,\n",
       " 'rockbottomradfm': 4,\n",
       " 'pulwama': 4,\n",
       " 'kosciusko': 4,\n",
       " 'nasasolarsystem': 4,\n",
       " 'autoinsurance': 4,\n",
       " 'bradleybrad': 4,\n",
       " 'michelebachman': 4,\n",
       " 'batfanuk': 4,\n",
       " 'rpics': 4,\n",
       " 'strikesstrikes': 4,\n",
       " 'montetjwitter': 4,\n",
       " 'kxlpkfa': 4,\n",
       " 'hermancranston': 4,\n",
       " 'letsfootball': 4,\n",
       " 'skanndtyagi': 4,\n",
       " 'fpmr': 4,\n",
       " 'njenga': 4,\n",
       " 'oooooohhhh': 4,\n",
       " 'tsunamiesh': 4,\n",
       " 'propertycasualty': 4,\n",
       " 'amageddon': 4,\n",
       " 'ameenshaikh': 4,\n",
       " 'alexbelloli': 4,\n",
       " 'walerga': 4,\n",
       " 'billneelynbc': 4,\n",
       " 'loseit': 4,\n",
       " 'diamorfiend': 4,\n",
       " 'newsgoldcoast': 4,\n",
       " 'yazidi': 4,\n",
       " 'weyreygidi': 4,\n",
       " '\\x89û¢åê': 4,\n",
       " \"'demolition\": 4,\n",
       " 'achimota': 4,\n",
       " 'liveonline': 4,\n",
       " 'kuug': 4,\n",
       " 'luchaunderground': 4,\n",
       " 'fatalityus': 4,\n",
       " 'techesback': 4,\n",
       " 'abuseddesolateamplost': 4,\n",
       " 'diesis': 4,\n",
       " 'ejxolpu': 4,\n",
       " 'ycnd': 4,\n",
       " 'blutz': 4,\n",
       " 'sixpenceee': 4,\n",
       " 'volcanoinrussia': 4,\n",
       " 'mosksn': 4,\n",
       " 'linerless': 4,\n",
       " 'aftershockdelo': 4,\n",
       " 'mkcbkcov': 4,\n",
       " 'whitbourne': 4,\n",
       " 'screamqueens': 4,\n",
       " 'politifiact': 4,\n",
       " 'notexplained': 4,\n",
       " 'hdetg': 4,\n",
       " 'rapidcity': 4,\n",
       " 'britishbakeoff': 4,\n",
       " 'southdowns': 4,\n",
       " 'treeporn': 4,\n",
       " 'hurricanedolce': 4,\n",
       " 'redeemeth': 4,\n",
       " 'efak': 4,\n",
       " 'generalnews': 4,\n",
       " 'jamaicaplain': 4,\n",
       " 'glink': 4,\n",
       " 'corleonedaboss': 4,\n",
       " 'noonan@cindynoonan': 4,\n",
       " 'yahistorical': 4,\n",
       " 'undergroundrailraod': 4,\n",
       " 'udz': 4,\n",
       " 'peterjukes': 4,\n",
       " 'videoveranomtv': 4,\n",
       " 'noahanyname': 4,\n",
       " 'renison': 4,\n",
       " 'siskiyou': 4,\n",
       " 'cdcgov': 4,\n",
       " 'martinmj': 4,\n",
       " 'mukilteo': 4,\n",
       " 'fewmoretweets': 4,\n",
       " 'notley': 4,\n",
       " 'blizzheroes': 4,\n",
       " 'originalfunko': 4,\n",
       " 'detouring': 4,\n",
       " 'pcps': 4,\n",
       " 'minhazmerchant': 4,\n",
       " 'envw': 4,\n",
       " 'nickcocofree': 4,\n",
       " 'juliedicaro': 4,\n",
       " 'jdabe': 4,\n",
       " 'playthursdays': 4,\n",
       " 'themagickidraps': 4,\n",
       " 'pllolz': 4,\n",
       " 'wwwcbplawyers': 4,\n",
       " 'kindermorgan': 4,\n",
       " 'newsarama': 4,\n",
       " 'cityofcalgary': 4,\n",
       " 'jecfrv': 4,\n",
       " 'royalcarribean': 4,\n",
       " 'teamhendrick': 4,\n",
       " 'shizune': 4,\n",
       " 'vgbootcamp': 4,\n",
       " 'vabengal': 4,\n",
       " 'zss': 4,\n",
       " 'versethe': 4,\n",
       " 'multidimensi': 4,\n",
       " 'fukushimatepco': 4,\n",
       " 'indiannews': 4,\n",
       " 'timkaine': 4,\n",
       " 'tweetlikeitsseptember': 4,\n",
       " 'yycstorm': 4,\n",
       " 'cadfyi': 4,\n",
       " 'rdhorndale': 4,\n",
       " 'meteoearth': 4,\n",
       " 'hanpcr': 4,\n",
       " 'fevwarrior': 4,\n",
       " 'doublecups': 4,\n",
       " 'thisizbwright': 4,\n",
       " 'cafire': 4,\n",
       " 'eajelul': 4,\n",
       " 'samanthaturne': 4,\n",
       " 'bakeofffriends': 4,\n",
       " 'ushanka': 4,\n",
       " 'ruebs': 4,\n",
       " 'xqm': 4,\n",
       " 'greenharvard': 4,\n",
       " 'qpg': 4,\n",
       " 'pmarca': 4,\n",
       " 'wmur': 4,\n",
       " 'chesttorso': 4,\n",
       " 'jonathanferrell': 4,\n",
       " 'arsonistmusic': 4,\n",
       " 'protectdenaliwolves': 4,\n",
       " 'growingupblack': 4,\n",
       " 'itsjustinstuart': 4,\n",
       " 'camilacabello': 4,\n",
       " 'yonews': 4,\n",
       " 'nsmejj': 4,\n",
       " 'zjin': 4,\n",
       " 'muzzamil': 4,\n",
       " 'offr': 4,\n",
       " 'lnva': 4,\n",
       " 'suruc': 4,\n",
       " 'debatequestionswewanttohear': 4,\n",
       " 'wocowae': 4,\n",
       " 'ushed': 4,\n",
       " 'londonfire': 4,\n",
       " 'kowing': 4,\n",
       " 'beclearoncancer': 4,\n",
       " 'missionhills': 4,\n",
       " 'dailykos': 4,\n",
       " 'jonvoyage': 4,\n",
       " 'askcharley': 4,\n",
       " 'smantibatam': 4,\n",
       " 'ashayo': 4,\n",
       " 'livingsafely': 4,\n",
       " 'ymcglaun': 4,\n",
       " 'hannaph': 4,\n",
       " 'teslas': 4,\n",
       " 'jamesmelville': 4,\n",
       " 'wildhorses': 4,\n",
       " 'magner': 4,\n",
       " 'dieplease': 4,\n",
       " 'petitiontake': 4,\n",
       " 'rtamerica': 4,\n",
       " 'podgyw': 4,\n",
       " 'fatalities/x': 4,\n",
       " 'eudrylantiqua': 4,\n",
       " 'kotaweather': 4,\n",
       " 'mishacollins': 4,\n",
       " 'dambisa': 4,\n",
       " 'prosyn': 4,\n",
       " 'insas': 4,\n",
       " 'aogashima': 4,\n",
       " 'stormchase': 4,\n",
       " 'sicroaanz': 4,\n",
       " 'chonce': 4,\n",
       " 'minimehh': 4,\n",
       " 'cjoyner': 4,\n",
       " 'rightways': 4,\n",
       " 'rightwaystan': 4,\n",
       " 'thetwister': 4,\n",
       " 'newzsacramento': 4,\n",
       " 'teamstream': 4,\n",
       " 'meinlcymbals': 4,\n",
       " 'beckarnley': 4,\n",
       " 'tomcatarts': 4,\n",
       " '==': 4,\n",
       " 'rohnertparkdps': 4,\n",
       " 'ebj': 4,\n",
       " 'countynews': 4,\n",
       " 'engvaus': 4,\n",
       " 'ltlt': 4,\n",
       " 'popularmmos': 4,\n",
       " 'theadvocatemag': 4,\n",
       " 'lavenderpoetrycafe': 4,\n",
       " 'njturnpike': 4,\n",
       " 'pmharper': 4,\n",
       " 'jamaicaobserver': 4,\n",
       " 'cnewslive': 4,\n",
       " 'wordpressdotcom': 4,\n",
       " 'pattonoswalt': 4,\n",
       " 'losdelsonido': 4,\n",
       " 'ivanberroa': 4,\n",
       " 'ubj': 4,\n",
       " 'softenza': 4,\n",
       " 'schwarber': 4,\n",
       " 'wpri': 4,\n",
       " 'megynkelly': 4,\n",
       " 'weallheartonedirection': 4,\n",
       " 'zippednews': 4,\n",
       " \"'t\": 4,\n",
       " 'zabadani': 4,\n",
       " 'msica': 4,\n",
       " 'alllivesmatter': 4,\n",
       " 'gmmbc': 4,\n",
       " 'icelandreview': 4,\n",
       " 'shawie': 4,\n",
       " 'infoorder': 4,\n",
       " 'iclown': 4,\n",
       " 'wzk': 4,\n",
       " 'insubcontinent': 4,\n",
       " \"'@foxy__siren\": 4,\n",
       " 'suryaray': 4,\n",
       " 'ariaahrary': 4,\n",
       " 'thetawniest': 4,\n",
       " 'japton': 4,\n",
       " 'jhaustin': 4,\n",
       " 'standwithpp': 4,\n",
       " 'routecomplex': 4,\n",
       " 'scvsupremecourt': 2,\n",
       " 'zqzgp': 2,\n",
       " 'datd': 2,\n",
       " 'yggzziz': 2,\n",
       " 'nkkcoh': 2,\n",
       " 'morningampi': 2,\n",
       " 'tellyampi': 2,\n",
       " 'bangampmy': 2,\n",
       " 'dadwho': 2,\n",
       " 'legwalked': 2,\n",
       " 'pqcmqchng': 2,\n",
       " 'grtvnews': 2,\n",
       " 'qibvv': 2,\n",
       " 'uhmmmm': 2,\n",
       " 'mylittlepwnies': 2,\n",
       " 'earlymay': 2,\n",
       " 'anathemazhiv': 2,\n",
       " 'tonysandos': 2,\n",
       " 'xdescry': 2,\n",
       " 'peiu': 2,\n",
       " 'ufcc': 2,\n",
       " 'timmicallef': 2,\n",
       " 'troubleonmymind': 2,\n",
       " 'jfreteii': 2,\n",
       " 'dzgzqemf': 2,\n",
       " 'drrichardbesser': 2,\n",
       " 'missleylaha': 2,\n",
       " 'rorington': 2,\n",
       " 'vpaoo': 2,\n",
       " 'uheyfdo': 2,\n",
       " 'cunayyh': 2,\n",
       " 'taufikcj': 2,\n",
       " 'angusmacneilsnp': 2,\n",
       " 'zqrsaqrrt': 2,\n",
       " 'ukvsxw': 2,\n",
       " 'cjmag': 2,\n",
       " 'yhbb': 2,\n",
       " 'hkf': 2,\n",
       " 'mwj': 2,\n",
       " 'rdxcfgqem': 2,\n",
       " 'ievn': 2,\n",
       " 'hellotybeeren': 2,\n",
       " 'handside': 2,\n",
       " 'daviesmutia': 2,\n",
       " 'zuji': 2,\n",
       " 'browserhijacker': 2,\n",
       " 'rogfjri': 2,\n",
       " 'stavola': 2,\n",
       " 'lhmovie': 2,\n",
       " 'oooureli': 2,\n",
       " 'catfishmtv': 2,\n",
       " 'qzloremft': 2,\n",
       " 'egyk': 2,\n",
       " 'jpz': 2,\n",
       " 'peterhowenecn': 2,\n",
       " 'eejyjky': 2,\n",
       " 'petebests': 2,\n",
       " 'dessicated': 2,\n",
       " 'modestmouseremix': 2,\n",
       " 'sizarg': 2,\n",
       " 'iizvyqyc': 2,\n",
       " 'twitsandiego': 2,\n",
       " 'cameronciletti': 2,\n",
       " 'tigersjostun': 2,\n",
       " 'qqvm': 2,\n",
       " 'rivieres': 2,\n",
       " 'legionstrackandfield': 2,\n",
       " 'xnznqb': 2,\n",
       " 'dvdssofn': 2,\n",
       " 'victoriagittins': 2,\n",
       " 'jcibenckz': 2,\n",
       " 'rbnhmtd': 2,\n",
       " 'safyuan': 2,\n",
       " 'fiendnikki': 2,\n",
       " 'scalpium': 2,\n",
       " 'wseptxga': 2,\n",
       " 'graysondolan': 2,\n",
       " 'xhlbxji': 2,\n",
       " 'eqmtkrl': 2,\n",
       " 'gnwt': 2,\n",
       " 'xvtj': 2,\n",
       " 'wsls': 2,\n",
       " 'jennasjems': 2,\n",
       " 'patrickwsls': 2,\n",
       " 'joelsherman': 2,\n",
       " 'gizrucy': 2,\n",
       " 'ohmygoshi': 2,\n",
       " 'ttdvjwe': 2,\n",
       " 'uyamdu': 2,\n",
       " 'rdwyjwu': 2,\n",
       " 'mkzpzfkl': 2,\n",
       " 'ictyosaur': 2,\n",
       " 'atomicbomb': 2,\n",
       " 'iecc': 2,\n",
       " 'cantmakeitup': 2,\n",
       " 'gpmhq': 2,\n",
       " 'adamnibloe': 2,\n",
       " 'cqnow': 2,\n",
       " 'mawmn': 2,\n",
       " 'cwheate': 2,\n",
       " 'ryleedowns': 2,\n",
       " 'nevaehburton': 2,\n",
       " 'uriminzok': 2,\n",
       " 'naemolgo': 2,\n",
       " 'yiraneuni': 2,\n",
       " 'wrongdejavu': 2,\n",
       " 'mwxyp': 2,\n",
       " 'vmmf': 2,\n",
       " 'pykyo': 2,\n",
       " 'bafdxey': 2,\n",
       " 'bestcomedyvine': 2,\n",
       " 'kworbbt': 2,\n",
       " 'orchardalley': 2,\n",
       " 'abcnorio': 2,\n",
       " 'rrxugsg': 2,\n",
       " 'hlvrkwgip': 2,\n",
       " 'joinvroom': 2,\n",
       " 'ohhhh': 2,\n",
       " 'tangletalk': 2,\n",
       " 'zvwadj': 2,\n",
       " 'nbyao': 2,\n",
       " 'pmdmejs': 2,\n",
       " 'worseits': 2,\n",
       " 'riverroaming': 2,\n",
       " 'alexhammerstone': 2,\n",
       " 'kttape': 2,\n",
       " 'ktfounder': 2,\n",
       " 'remymarcel': 2,\n",
       " 'frofrofro': 2,\n",
       " 'kbltcxz': 2,\n",
       " 'zykct': 2,\n",
       " 'bannukes': 2,\n",
       " 'thoutaylorbrown': 2,\n",
       " 'fcmupm': 2,\n",
       " 'mzx': 2,\n",
       " 'dskf': 2,\n",
       " 'ivnfg': 2,\n",
       " 'entension': 2,\n",
       " 'apcpdp': 2,\n",
       " 'ufeibala': 2,\n",
       " 'ennullkzm': 2,\n",
       " 'fuelgas': 2,\n",
       " 'tahoeblazeravalanches': 2,\n",
       " 'tqc': 2,\n",
       " 'anarchicteapot': 2,\n",
       " 'boironusa': 2,\n",
       " 'glononium': 2,\n",
       " 'nitroglycerin': 2,\n",
       " 'wjrvx': 2,\n",
       " 'stepkans': 2,\n",
       " 'lvictoria': 2,\n",
       " 'ztyg': 2,\n",
       " 'pioneerpress': 2,\n",
       " 'fhucxhl': 2,\n",
       " 'asgrjswc': 2,\n",
       " '..do': 2,\n",
       " 'freemarketeer': 2,\n",
       " 'dibang': 2,\n",
       " 'sbee': 2,\n",
       " 'kqksosw': 2,\n",
       " 'hvzlaradio': 2,\n",
       " 'bakpnyn': 2,\n",
       " 'ofsv': 2,\n",
       " 'paovyzttw': 2,\n",
       " 'bpieyms': 2,\n",
       " 'eruep': 2,\n",
       " 'fcqo': 2,\n",
       " 'qshhnb': 2,\n",
       " 'nybw': 2,\n",
       " 'trjdavis': 2,\n",
       " 'zzcupnz': 2,\n",
       " 'tellyfckngo': 2,\n",
       " 'jaycootchi': 2,\n",
       " 'babality': 2,\n",
       " 'dpmv': 2,\n",
       " 'vcfs': 2,\n",
       " 'phdsquares': 2,\n",
       " 'dirknomissski': 2,\n",
       " 'moorlandschmbr': 2,\n",
       " 'slxhw': 2,\n",
       " 'deltachildren': 2,\n",
       " 'zwose': 2,\n",
       " 'fyzp': 2,\n",
       " 'xnplio': 2,\n",
       " 'youssefyamani': 2,\n",
       " 'swellyjetevo': 2,\n",
       " 'lwq': 2,\n",
       " 'foqjzw': 2,\n",
       " 'fqc': 2,\n",
       " 'dyxtmrydu': 2,\n",
       " 'oroyunym': 2,\n",
       " 'jnxz': 2,\n",
       " 'qxcffh': 2,\n",
       " 'xnecd': 2,\n",
       " 'fatburning': 2,\n",
       " 'burnfat': 2,\n",
       " 'sdhhl': 2,\n",
       " 'yvulizbk': 2,\n",
       " 'hripsk': 2,\n",
       " 'jittering': 2,\n",
       " 'lordbrathwaite': 2,\n",
       " 'growingupincolorado': 2,\n",
       " 'esbsa': 2,\n",
       " 'utfire': 2,\n",
       " 'peterknox': 2,\n",
       " 'gemmasterful': 2,\n",
       " 'lakeeffect': 2,\n",
       " 'umqh': 2,\n",
       " 'huqundqi': 2,\n",
       " 'wfxbaqmbk': 2,\n",
       " 'hailthe': 2,\n",
       " 'ulj': 2,\n",
       " 'jamilazzaini': 2,\n",
       " 'alifaditha': 2,\n",
       " 'nervana': 2,\n",
       " 'dntx': 2,\n",
       " 'cvegtizog': 2,\n",
       " 'fcqgr': 2,\n",
       " 'twgpmm': 2,\n",
       " 'pmmtzlwp': 2,\n",
       " 'msnzv': 2,\n",
       " 'pbvio': 2,\n",
       " 'toddstarnes': 2,\n",
       " 'lfzhxn': 2,\n",
       " 'jakartapost': 2,\n",
       " 'capeann': 2,\n",
       " 'triciaoneill': 2,\n",
       " 'triciaoneillphoto': 2,\n",
       " 'qdi': 2,\n",
       " 'cmagasi': 2,\n",
       " 'xovp': 2,\n",
       " 'atlbizchron': 2,\n",
       " 'audyha': 2,\n",
       " 'ekgc': 2,\n",
       " 'backty': 2,\n",
       " 'uqkop': 2,\n",
       " 'destinationimpact': 2,\n",
       " 'stkcv': 2,\n",
       " 'jxbiej': 2,\n",
       " 'benstracy': 2,\n",
       " 'owia': 2,\n",
       " 'elecman': 2,\n",
       " 'rantipozi': 2,\n",
       " 'cucks': 2,\n",
       " 'qogmoegu': 2,\n",
       " 'vqxdouvt': 2,\n",
       " 'atljw': 2,\n",
       " 'messnermatthew': 2,\n",
       " 'newcity': 2,\n",
       " 'towboat': 2,\n",
       " 'thesensualeye': 2,\n",
       " 'kgawp': 2,\n",
       " 'stuckinbooks': 2,\n",
       " 'ouxq': 2,\n",
       " 'vuf': 2,\n",
       " 'screams*do': 2,\n",
       " 'animatronics': 2,\n",
       " 'relaxinpr': 2,\n",
       " 'miprv': 2,\n",
       " 'brduau': 2,\n",
       " 'pcntczoxv': 2,\n",
       " 'akjm': 2,\n",
       " 'annmarieronan': 2,\n",
       " 'niamhosullivanx': 2,\n",
       " 'postering': 2,\n",
       " 'calgaryfringe': 2,\n",
       " 'yycfringe': 2,\n",
       " 'killhard': 2,\n",
       " 'dailytimesngr': 2,\n",
       " 'jampampk': 2,\n",
       " 'oliviaapalmerr': 2,\n",
       " 'womengirls': 2,\n",
       " 'eversafe': 2,\n",
       " 'ca\\x89û': 2,\n",
       " 'sunnymeade': 2,\n",
       " 'fijankjb': 2,\n",
       " 'djllxozp': 2,\n",
       " 'lbtshxi': 2,\n",
       " 'jmkdtcymj': 2,\n",
       " 'ejl': 2,\n",
       " 'lzlch': 2,\n",
       " 'starmade': 2,\n",
       " 'hhviumtm': 2,\n",
       " 'catsandsyrup': 2,\n",
       " 'swlxmr': 2,\n",
       " 'ageekyfangirl': 2,\n",
       " 'astrologian': 2,\n",
       " 'azai': 2,\n",
       " 'chargedup': 2,\n",
       " 'rstormcoming': 2,\n",
       " 'intactmh': 2,\n",
       " 'partliftsoddsplaneglidednotcrashedintosea': 2,\n",
       " 'qbgos': 2,\n",
       " 'yahoofinancehope': 2,\n",
       " 'plkedhr': 2,\n",
       " 'pvvt': 2,\n",
       " 'seanpeconi': 2,\n",
       " 'jasonfloyd': 2,\n",
       " 'lynchonsports': 2,\n",
       " 'criscyborg': 2,\n",
       " 'vdbv': 2,\n",
       " 'pusvjln': 2,\n",
       " 'rjailbreak': 2,\n",
       " 'tpnmbjvg': 2,\n",
       " 'lesleymariiee': 2,\n",
       " 'dratomic': 2,\n",
       " 'dustpiggies': 2,\n",
       " 'dustpig': 2,\n",
       " 'bamenda': 2,\n",
       " 'coidv': 2,\n",
       " 'shaiqiw': 2,\n",
       " 'austinpearcy': 2,\n",
       " 'fleek': 2,\n",
       " 'schqfest': 2,\n",
       " 'kristindavis': 2,\n",
       " 'vmzoj': 2,\n",
       " 'zqlk': 2,\n",
       " 'wwwcn': 2,\n",
       " 'prisonplanet': 2,\n",
       " 'warningwild': 2,\n",
       " 'szwlwl': 2,\n",
       " 'blazerfan': 2,\n",
       " 'ignoranceshe': 2,\n",
       " 'latinoand': 2,\n",
       " 'benothing': 2,\n",
       " 'morebut': 2,\n",
       " 'broseidonrex': 2,\n",
       " 'dapurplesharpie': 2,\n",
       " 'driverlesscars': 2,\n",
       " 'ffjt': 2,\n",
       " 'vcrrx': 2,\n",
       " 'nvf': 2,\n",
       " 'fxmn': 2,\n",
       " 'homeworldgym': 2,\n",
       " 'thisisperidot': 2,\n",
       " 'trulystings': 2,\n",
       " 'santaclara': 2,\n",
       " 'kurtkamka': 2,\n",
       " 'pvjvdpf': 2,\n",
       " 'daorcey': 2,\n",
       " 'graywardens': 2,\n",
       " 'sonofbaldwin': 2,\n",
       " 'bookslast': 2,\n",
       " 'checkedhe': 2,\n",
       " 'niagaravehicles': 2,\n",
       " 'tkyonly': 2,\n",
       " 'duckvillelol': 2,\n",
       " 'nwqdje': 2,\n",
       " 'runjewels': 2,\n",
       " 'vwovhs': 2,\n",
       " 'sxki': 2,\n",
       " 'msmigot': 2,\n",
       " 'givingevidence': 2,\n",
       " 'lzj': 2,\n",
       " 'jcyxhq': 2,\n",
       " 'siouxland': 2,\n",
       " 'siouxlan': 2,\n",
       " 'mtwh': 2,\n",
       " 'jjns': 2,\n",
       " 'wackoes': 2,\n",
       " 'myvintagesoul': 2,\n",
       " 'leedstraif': 2,\n",
       " 'bxzzhxkm': 2,\n",
       " 'mythgriy': 2,\n",
       " 'rjekvddp': 2,\n",
       " 'disabledveterans': 2,\n",
       " 'thepartyofmeanness': 2,\n",
       " 'shantaeforsmash': 2,\n",
       " 'shantaehalfgeniehero': 2,\n",
       " 'sosfamupdater': 2,\n",
       " 'trhw': 2,\n",
       " 'whitewalkers': 2,\n",
       " 'joegoodmanjr': 2,\n",
       " 'marcholl': 2,\n",
       " 'nennicook': 2,\n",
       " 'aitchkaycee': 2,\n",
       " 'vixstuart': 2,\n",
       " 'benjbeckwith': 2,\n",
       " 'rksp': 2,\n",
       " 'wqzaor': 2,\n",
       " 'ieldg': 2,\n",
       " 'accidentalprophecy': 2,\n",
       " 'vzyt': 2,\n",
       " 'hkw': 2,\n",
       " 'qlppt': 2,\n",
       " 'zjssft': 2,\n",
       " 'aqrnankfd': 2,\n",
       " 'ksbynews': 2,\n",
       " 'ojhpdfg': 2,\n",
       " 'ampstart': 2,\n",
       " 'ampask': 2,\n",
       " 'vepu': 2,\n",
       " 'ynebn': 2,\n",
       " 'fcjyec': 2,\n",
       " 'mzycu': 2,\n",
       " 'nghlth': 2,\n",
       " 'prayyc': 2,\n",
       " 'brokenscnecal': 2,\n",
       " 'hngnhvb': 2,\n",
       " 'nolesfan': 2,\n",
       " 'nutsandboltssp': 2,\n",
       " 'transwomen': 2,\n",
       " 'csismica': 2,\n",
       " 'dbtsu': 2,\n",
       " 'pitmix': 2,\n",
       " 'vovktxspo': 2,\n",
       " 'scriptettesar': 2,\n",
       " 'katiecool': 2,\n",
       " 'njpor': 2,\n",
       " 'blazingben': 2,\n",
       " 'pattyds': 2,\n",
       " 'gwfrazee': 2,\n",
       " 'joshuaassaraf': 2,\n",
       " 'bookanother': 2,\n",
       " 'senfeinstein': 2,\n",
       " 'modelbubbles': 2,\n",
       " 'mrobama': 2,\n",
       " 'yjb': 2,\n",
       " 'tcpm': 2,\n",
       " 'qddn': 2,\n",
       " 'kabarmesir': 2,\n",
       " 'rememberrabaa': 2,\n",
       " 'vigilent': 2,\n",
       " 'nativehuman': 2,\n",
       " 'myreligion': 2,\n",
       " 'rwjfvl': 2,\n",
       " 'joboozoso': 2,\n",
       " 'usatodaynfl': 2,\n",
       " 'gcej': 2,\n",
       " 'rottentomatoes': 2,\n",
       " 'uxkizwm': 2,\n",
       " 'josephjett': 2,\n",
       " 'itssselenaluna': 2,\n",
       " 'jxx': 2,\n",
       " 'devinjoslyn': 2,\n",
       " 'kuylcxg': 2,\n",
       " 'mumbaitimes': 2,\n",
       " 'matakomilk': 2,\n",
       " 'greenbuildermag': 2,\n",
       " 'firewise': 2,\n",
       " 'michelenfpa': 2,\n",
       " 'nczlif': 2,\n",
       " 'plht': 2,\n",
       " 'wxnpj': 2,\n",
       " 'eokggb': 2,\n",
       " 'iyzsda': 2,\n",
       " 'moejxqya': 2,\n",
       " 'lastingness': 2,\n",
       " 'ziuw': 2,\n",
       " 'kkgsjx': 2,\n",
       " 'toddcalfee': 2,\n",
       " 'mattburgener': 2,\n",
       " 'hauhei': 2,\n",
       " 'ojoubot': 2,\n",
       " 'justintrudeau': 2,\n",
       " 'khqa': 2,\n",
       " 'jdnijx': 2,\n",
       " 'gisuserpr': 2,\n",
       " 'geotech': 2,\n",
       " 'lawsonofficial': 2,\n",
       " 'womem': 2,\n",
       " 'mommyisbomb': 2,\n",
       " 'gakx': 2,\n",
       " 'zhenghxn': 2,\n",
       " 'uspacific': 2,\n",
       " 'lifeaintfairkid': 2,\n",
       " 'brooo': 2,\n",
       " 'xeuj': 2,\n",
       " 'ldx': 2,\n",
       " 'truediagnosis': 2,\n",
       " 'detroitpls': 2,\n",
       " 'vsrt': 2,\n",
       " 'asvzh': 2,\n",
       " 'adanne': 2,\n",
       " 'soonermagic': 2,\n",
       " 'thatpersianguy': 2,\n",
       " 'youngsafe': 2,\n",
       " 'njvvtygn': 2,\n",
       " 'aleisstokes': 2,\n",
       " 'intelligencebar': 2,\n",
       " 'jrtl': 2,\n",
       " 'qnxb': 2,\n",
       " 'fkhanage': 2,\n",
       " 'kwof': 2,\n",
       " 'scraptrident': 2,\n",
       " 'xfpv': 2,\n",
       " 'crqck': 2,\n",
       " 'becyme': 2,\n",
       " 'kamunt': 2,\n",
       " 'youuu': 2,\n",
       " 'qwyeeec': 2,\n",
       " 'deathholy': 2,\n",
       " 'molys': 2,\n",
       " 'propertycasu': 2,\n",
       " 'vupg': 2,\n",
       " 'firstpostin': 2,\n",
       " 'labourleadership': 2,\n",
       " 'daborsch': 2,\n",
       " 'fzfh': 2,\n",
       " 'dattomm': 2,\n",
       " 'xlywbn': 2,\n",
       " 'bobbyofhomewood': 2,\n",
       " 'joxroundtable': 2,\n",
       " 'gravitymovie': 2,\n",
       " 'poconorecord': 2,\n",
       " 'emergencymgtmag': 2,\n",
       " 'zlsumiess': 2,\n",
       " 'eukt': 2,\n",
       " 'ebpybfh': 2,\n",
       " 'randerson': 2,\n",
       " 'strickskin': 2,\n",
       " 'nickscomics': 2,\n",
       " 'ncvjovc': 2,\n",
       " 'lamdj': 2,\n",
       " 'cncpts': 2,\n",
       " 'solelinks': 2,\n",
       " 'cinla': 2,\n",
       " 'windowgatribble': 2,\n",
       " 'ieee': 2,\n",
       " 'kircut': 2,\n",
       " 'dangerousbeans': 2,\n",
       " 'cssr': 2,\n",
       " 'ftmu': 2,\n",
       " 'bvkt': 2,\n",
       " 'sureshpprabhu': 2,\n",
       " 'gsbnu': 2,\n",
       " 'mordechai': 2,\n",
       " 'shemesh': 2,\n",
       " 'edwelchmusic': 2,\n",
       " 'chemicalbabe': 2,\n",
       " 'idwx': 2,\n",
       " 'pacic': 2,\n",
       " 'bmie': 2,\n",
       " 'whensoever': 2,\n",
       " 'sizygwwf': 2,\n",
       " 'zicjudxo': 2,\n",
       " 'xlbbdr': 2,\n",
       " 'stury': 2,\n",
       " 'jsjfftc': 2,\n",
       " 'dywy': 2,\n",
       " 'xyqd': 2,\n",
       " 'bangmeupguk': 2,\n",
       " 'caitsroberts': 2,\n",
       " 'fixingmatch': 2,\n",
       " 'hvgwd': 2,\n",
       " 'halfhourhotel': 2,\n",
       " 'edgarsgift': 2,\n",
       " 'splatoon': 2,\n",
       " 'splattershot': 2,\n",
       " 'ulqu': 2,\n",
       " 'juliechen': 2,\n",
       " 'keoe': 2,\n",
       " 'referencereference': 2,\n",
       " 'xekstrin': 2,\n",
       " 'yyjlukfj': 2,\n",
       " 'abrancaballero': 2,\n",
       " 'bexkgwr': 2,\n",
       " 'qhqo': 2,\n",
       " 'ejtu': 2,\n",
       " 'qybkzol': 2,\n",
       " 'creelyou': 2,\n",
       " 'dobut': 2,\n",
       " 'chromsucks': 2,\n",
       " 'rwwe': 2,\n",
       " 'wakeupflorida': 2,\n",
       " 'killedinjured': 2,\n",
       " 'trophyhunt': 2,\n",
       " 'mexaj': 2,\n",
       " 'capicapricapri': 2,\n",
       " 'brentobento': 2,\n",
       " 'francisunderwood': 2,\n",
       " 'nuvytu': 2,\n",
       " 'fionagilbert': 2,\n",
       " 'arzyi': 2,\n",
       " 'wolforth': 2,\n",
       " 'phnpy': 2,\n",
       " 'quux': 2,\n",
       " 'nbcnightlynews': 2,\n",
       " 'princeoffencing': 2,\n",
       " 'blanksocietyx': 2,\n",
       " 'rachelrofe': 2,\n",
       " 'perkpearl': 2,\n",
       " 'orshow': 2,\n",
       " 'ucnzh': 2,\n",
       " 'fwymw': 2,\n",
       " 'kpkt': 2,\n",
       " 'rosemarytravale': 2,\n",
       " 'qtvdsoh': 2,\n",
       " 'drsarwatzaib': 2,\n",
       " 'mcourt': 2,\n",
       " 'nwhdky': 2,\n",
       " 'thestrain': 2,\n",
       " 'wickett': 2,\n",
       " 'knnwugwt': 2,\n",
       " 'charleyisqueen': 2,\n",
       " 'bqjtp': 2,\n",
       " 'zqajs': 2,\n",
       " 'bathroomits': 2,\n",
       " 'louderthings': 2,\n",
       " 'missjadebrown': 2,\n",
       " 'aoni': 2,\n",
       " 'sputnikint': 2,\n",
       " 'mkav': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = my_vocabulary.oov\n",
    "x = {k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "total = 0\n",
    "for i,j in x.items():\n",
    "    total += j\n",
    "print(total)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, scheduler, name):\n",
    "    train_state = {    \n",
    "    'model' : model,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(train_state, name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_cf.parameters(), lr=0.01, weight_decay = 0.015);\n",
    "# optimizer = torch.optim.SGD(lstm_cf.parameters(), lr=0.001, weight_decay = 0.005) \n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'max', factor = 0.1, patience = 10, verbose=True)\n",
    "\n",
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 0.018\n",
      "[1,    60] loss: 0.016\n",
      "Accuracy on EPOCH 1 test images: 78 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.782563\n",
      "[2,    30] loss: 0.015\n",
      "[2,    60] loss: 0.014\n",
      "Accuracy on EPOCH 2 test images: 79 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.792542\n",
      "[3,    30] loss: 0.014\n",
      "[3,    60] loss: 0.015\n",
      "Accuracy on EPOCH 3 test images: 80 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.800420\n",
      "[4,    30] loss: 0.015\n",
      "[4,    60] loss: 0.014\n",
      "Accuracy on EPOCH 4 test images: 78 %\n",
      "[5,    30] loss: 0.014\n",
      "[5,    60] loss: 0.014\n",
      "Accuracy on EPOCH 5 test images: 79 %\n",
      "[6,    30] loss: 0.014\n",
      "[6,    60] loss: 0.015\n",
      "Accuracy on EPOCH 6 test images: 76 %\n",
      "[7,    30] loss: 0.015\n",
      "[7,    60] loss: 0.014\n",
      "Accuracy on EPOCH 7 test images: 80 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.804622\n",
      "[8,    30] loss: 0.014\n",
      "[8,    60] loss: 0.015\n",
      "Accuracy on EPOCH 8 test images: 74 %\n",
      "[9,    30] loss: 0.015\n",
      "[9,    60] loss: 0.014\n",
      "Accuracy on EPOCH 9 test images: 79 %\n",
      "[10,    30] loss: 0.014\n",
      "[10,    60] loss: 0.014\n",
      "Accuracy on EPOCH 10 test images: 79 %\n",
      "[11,    30] loss: 0.014\n",
      "[11,    60] loss: 0.015\n",
      "Accuracy on EPOCH 11 test images: 80 %\n",
      "[12,    30] loss: 0.014\n",
      "[12,    60] loss: 0.015\n",
      "Accuracy on EPOCH 12 test images: 78 %\n",
      "[13,    30] loss: 0.014\n",
      "[13,    60] loss: 0.014\n",
      "Accuracy on EPOCH 13 test images: 77 %\n",
      "[14,    30] loss: 0.014\n",
      "[14,    60] loss: 0.014\n",
      "Accuracy on EPOCH 14 test images: 79 %\n",
      "[15,    30] loss: 0.014\n",
      "[15,    60] loss: 0.014\n",
      "Accuracy on EPOCH 15 test images: 79 %\n",
      "[16,    30] loss: 0.014\n",
      "[16,    60] loss: 0.014\n",
      "Accuracy on EPOCH 16 test images: 79 %\n",
      "[17,    30] loss: 0.014\n",
      "[17,    60] loss: 0.014\n",
      "Accuracy on EPOCH 17 test images: 79 %\n",
      "[18,    30] loss: 0.014\n",
      "[18,    60] loss: 0.014\n",
      "Accuracy on EPOCH 18 test images: 78 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "[19,    30] loss: 0.014\n",
      "[19,    60] loss: 0.013\n",
      "Accuracy on EPOCH 19 test images: 80 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.806723\n",
      "[20,    30] loss: 0.012\n",
      "[20,    60] loss: 0.013\n",
      "Accuracy on EPOCH 20 test images: 80 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.807773\n",
      "[21,    30] loss: 0.013\n",
      "[21,    60] loss: 0.013\n",
      "Accuracy on EPOCH 21 test images: 80 %\n",
      "[22,    30] loss: 0.012\n",
      "[22,    60] loss: 0.013\n",
      "Accuracy on EPOCH 22 test images: 81 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.811975\n",
      "[23,    30] loss: 0.012\n",
      "[23,    60] loss: 0.012\n",
      "Accuracy on EPOCH 23 test images: 80 %\n",
      "[24,    30] loss: 0.012\n",
      "[24,    60] loss: 0.013\n",
      "Accuracy on EPOCH 24 test images: 81 %\n",
      "Saving current model!!!\n",
      "Detailed Accuracy: 0.816702\n",
      "[25,    30] loss: 0.012\n",
      "[25,    60] loss: 0.013\n",
      "Accuracy on EPOCH 25 test images: 80 %\n",
      "[26,    30] loss: 0.012\n",
      "[26,    60] loss: 0.012\n",
      "Accuracy on EPOCH 26 test images: 81 %\n",
      "[27,    30] loss: 0.012\n",
      "[27,    60] loss: 0.012\n",
      "Accuracy on EPOCH 27 test images: 80 %\n",
      "[28,    30] loss: 0.012\n",
      "[28,    60] loss: 0.012\n",
      "Accuracy on EPOCH 28 test images: 81 %\n",
      "[29,    30] loss: 0.012\n",
      "[29,    60] loss: 0.012\n",
      "Accuracy on EPOCH 29 test images: 80 %\n",
      "[30,    30] loss: 0.012\n",
      "[30,    60] loss: 0.012\n",
      "Accuracy on EPOCH 30 test images: 80 %\n",
      "[31,    30] loss: 0.011\n",
      "[31,    60] loss: 0.012\n",
      "Accuracy on EPOCH 31 test images: 81 %\n",
      "[32,    30] loss: 0.012\n",
      "[32,    60] loss: 0.012\n",
      "Accuracy on EPOCH 32 test images: 81 %\n",
      "[33,    30] loss: 0.012\n",
      "[33,    60] loss: 0.012\n",
      "Accuracy on EPOCH 33 test images: 81 %\n",
      "[34,    30] loss: 0.012\n",
      "[34,    60] loss: 0.012\n",
      "Accuracy on EPOCH 34 test images: 81 %\n",
      "[35,    30] loss: 0.012\n",
      "[35,    60] loss: 0.012\n",
      "Accuracy on EPOCH 35 test images: 81 %\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[36,    30] loss: 0.012\n",
      "[36,    60] loss: 0.012\n",
      "Accuracy on EPOCH 36 test images: 81 %\n",
      "[37,    30] loss: 0.012\n",
      "[37,    60] loss: 0.011\n",
      "Accuracy on EPOCH 37 test images: 81 %\n",
      "[38,    30] loss: 0.012\n",
      "[38,    60] loss: 0.011\n",
      "Accuracy on EPOCH 38 test images: 81 %\n",
      "[39,    30] loss: 0.011\n",
      "[39,    60] loss: 0.012\n",
      "Accuracy on EPOCH 39 test images: 81 %\n",
      "[40,    30] loss: 0.011\n",
      "[40,    60] loss: 0.012\n",
      "Accuracy on EPOCH 40 test images: 81 %\n",
      "[41,    30] loss: 0.011\n",
      "[41,    60] loss: 0.012\n",
      "Accuracy on EPOCH 41 test images: 81 %\n",
      "[42,    30] loss: 0.011\n",
      "[42,    60] loss: 0.012\n",
      "Accuracy on EPOCH 42 test images: 81 %\n",
      "[43,    30] loss: 0.011\n",
      "[43,    60] loss: 0.011\n",
      "Accuracy on EPOCH 43 test images: 81 %\n",
      "[44,    30] loss: 0.012\n",
      "[44,    60] loss: 0.011\n",
      "Accuracy on EPOCH 44 test images: 81 %\n",
      "[45,    30] loss: 0.011\n",
      "[45,    60] loss: 0.012\n",
      "Accuracy on EPOCH 45 test images: 81 %\n",
      "[46,    30] loss: 0.012\n",
      "[46,    60] loss: 0.011\n",
      "Accuracy on EPOCH 46 test images: 81 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[47,    30] loss: 0.011\n",
      "[47,    60] loss: 0.011\n",
      "Accuracy on EPOCH 47 test images: 80 %\n",
      "[48,    30] loss: 0.011\n",
      "[48,    60] loss: 0.011\n",
      "Accuracy on EPOCH 48 test images: 81 %\n",
      "[49,    30] loss: 0.011\n",
      "[49,    60] loss: 0.011\n",
      "Accuracy on EPOCH 49 test images: 81 %\n",
      "[50,    30] loss: 0.012\n",
      "[50,    60] loss: 0.011\n",
      "Accuracy on EPOCH 50 test images: 80 %\n",
      "[51,    30] loss: 0.011\n",
      "[51,    60] loss: 0.011\n",
      "Accuracy on EPOCH 51 test images: 81 %\n",
      "[52,    30] loss: 0.012\n",
      "[52,    60] loss: 0.012\n",
      "Accuracy on EPOCH 52 test images: 81 %\n",
      "[53,    30] loss: 0.012\n",
      "[53,    60] loss: 0.011\n",
      "Accuracy on EPOCH 53 test images: 80 %\n",
      "[54,    30] loss: 0.011\n",
      "[54,    60] loss: 0.011\n",
      "Accuracy on EPOCH 54 test images: 81 %\n",
      "[55,    30] loss: 0.012\n",
      "[55,    60] loss: 0.012\n",
      "Accuracy on EPOCH 55 test images: 81 %\n",
      "[56,    30] loss: 0.011\n",
      "[56,    60] loss: 0.011\n",
      "Accuracy on EPOCH 56 test images: 81 %\n",
      "[57,    30] loss: 0.011\n",
      "[57,    60] loss: 0.011\n",
      "Accuracy on EPOCH 57 test images: 81 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[58,    30] loss: 0.012\n",
      "[58,    60] loss: 0.011\n",
      "Accuracy on EPOCH 58 test images: 81 %\n",
      "[59,    30] loss: 0.011\n",
      "[59,    60] loss: 0.012\n",
      "Accuracy on EPOCH 59 test images: 81 %\n",
      "[60,    30] loss: 0.012\n",
      "[60,    60] loss: 0.012\n",
      "Accuracy on EPOCH 60 test images: 81 %\n",
      "Finished Training\n",
      "CPU times: user 35min 9s, sys: 52.2 s, total: 36min 2s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EARLY_STOP = 0\n",
    "EPOCH = 60\n",
    "model_name = 'lstm.pt'\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "#     if EARLY_STOP >= 21:\n",
    "#         print(\"STOPPED EARLY!!\")\n",
    "#         break\n",
    "    lstm_cf.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_dataloader, 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs, _ = lstm_cf(batch_X)\n",
    "        loss = criterion(outputs, batch_y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    lstm_cf.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for test_X, test_y in test_dataloader:\n",
    "        outputs = lstm_cf.predict(test_X)\n",
    "        total += test_y.shape[0]\n",
    "        correct += outputs.squeeze().eq(test_y).sum().item()\n",
    "    cur_accuracy = correct / total\n",
    "    print('Accuracy on EPOCH %d test images: %d %%' % (epoch+1, 100 * cur_accuracy))\n",
    "    lr_scheduler.step(cur_accuracy)\n",
    "    if cur_accuracy > best_accuracy:\n",
    "            best_accuracy = cur_accuracy\n",
    "            print(\"Saving current model!!!\")\n",
    "            print(\"Detailed Accuracy: %f\" %(best_accuracy))\n",
    "            save_model(epoch, lstm_cf, optimizer, lr_scheduler, model_name)\n",
    "            EARLY_STOP = 0\n",
    "    EARLY_STOP += 1\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give best_classifier the class of the model you want to load\n",
    "best_classifier = lstm_cf\n",
    "\n",
    "best_classifier.load_state_dict(torch.load('lstm.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_input = torch.tensor(X_test_lstm).long()\n",
    "X_train_input = torch.tensor(X_train_lstm).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Test:\n",
      "0.8146008403361344\n"
     ]
    }
   ],
   "source": [
    "best_classifier.eval()\n",
    "preds = best_classifier.predict(X_test_input).detach().numpy()\n",
    "print(\"Accuracy - Test:\")\n",
    "print((preds.squeeze() == y_test).sum().item() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Train:\n",
      "0.837099316868103\n"
     ]
    }
   ],
   "source": [
    "best_classifier.eval()\n",
    "preds = best_classifier.predict(X_train_input).detach().numpy()\n",
    "print(\"Accuracy - Train:\")\n",
    "print((preds.squeeze() == y_train).sum().item() / y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "# df_test[\"text\"] = df_test[\"text\"].apply(lambda x: txt_cleanning(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_X = df_test[\"text\"].apply(txt_cleanning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-91e1adee7208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit_X_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmit_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vec' is not defined"
     ]
    }
   ],
   "source": [
    "submit_X_tfidf = tfidf_vec.transform(submit_X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [01:52<00:00, 29.04it/s]\n"
     ]
    }
   ],
   "source": [
    "submit_X_use = []\n",
    "\n",
    "for text in tqdm(submit_X):\n",
    "    submit_X_use.append(get_use(text, use_module).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_X_inputs, _ = create_numpy(submit_X, submit_X, my_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_preds = best_classifier.predict(torch.tensor(submit_X_inputs).long()).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_submit_df = pd.DataFrame(submit_preds, columns=[\"target\"])\n",
    "\n",
    "to_submit_df = pd.concat((df_test[\"id\"], to_submit_df), axis = 1)\n",
    "\n",
    "to_submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 22.2k/22.2k [00:05<00:00, 4.39kB/s]\n",
      "Successfully submitted to Real or Not? NLP with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started  -f submission.csv -m \"Test\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
